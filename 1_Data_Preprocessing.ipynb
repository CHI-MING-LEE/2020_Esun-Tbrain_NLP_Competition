{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_Data_Preprocessing",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_WEyl6qkRzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cghsv4bRip6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/t-brain_2020_NLP_team_share_folder\")\n",
        "# os.chdir(r\"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMiwXr8yi3aC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news_urls = pd.read_csv('raw_data/train_data_custom_0628.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OBlXb3l1D7R",
        "colab_type": "text"
      },
      "source": [
        "## 內文前處理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsY7CLaC1aJH",
        "colab_type": "text"
      },
      "source": [
        "### 去除英文字、標點符號、特殊符號"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFd97hkyN2rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 多加全形轉半形\n",
        "def strQ2B(ustring):\n",
        "    if isinstance(ustring, str) :\n",
        "      \"\"\"把字串全形轉半形\"\"\"\n",
        "      ss = []\n",
        "      for s in ustring:\n",
        "          rstring = \"\"\n",
        "          for uchar in s:\n",
        "              inside_code = ord(uchar)\n",
        "              if inside_code == 12288:  # 全形空格直接轉換\n",
        "                  inside_code = 32\n",
        "              elif (inside_code >= 65281 and inside_code <= 65374):  # 全形字元（除空格）根據關係轉化\n",
        "                  inside_code -= 65248\n",
        "              rstring += chr(inside_code)\n",
        "          ss.append(rstring)\n",
        "      return ''.join(ss)\n",
        "    else:\n",
        "      return unstring\n",
        "\n",
        "news_urls[\"content\"] = news_urls[\"content\"].apply(lambda x: re.sub(\"[▲◆▪【】*';％%※★<>#〈〉' '_|｜()（）－-]\", \"\", str(strQ2B(x)))) # !！？?。\n",
        "news_urls[\"content\"] = news_urls[\"content\"].apply(lambda x: re.sub(\"[!！？?]\", \"。\", str(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "359atSWg1KDW",
        "colab_type": "text"
      },
      "source": [
        "## 整理成Training Data格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2_uCqVrvBy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split(word):\n",
        "    return [char for char in word]\n",
        "\n",
        "start_idx = []\n",
        "end_idx = []\n",
        "name_list = []\n",
        "NRE_list = []\n",
        "\n",
        "for sun_name in truth:\n",
        "    # sun_name = truth[0]\n",
        "    sub_start = [m.start() for m in re.finditer(sun_name, input_context)]\n",
        "    sub_end = list(np.asarray(sub_start) + len(sun_name))\n",
        "    NRE_list += np.repeat('PERSON', len(sub_start)).tolist()\n",
        "    name_list += np.repeat(sun_name, len(sub_start)).tolist()\n",
        "    start_idx += sub_start\n",
        "    end_idx += sub_end\n",
        "entity_sentence_list = [set(zip(start_idx, end_idx, NRE_list, name_list))]\n",
        "\n",
        "review_PER = []\n",
        "for entity in sorted(entity_sentence_list[0]):\n",
        "    print(entity)\n",
        "    if entity[2] == 'PERSON':\n",
        "        review_PER.append(entity[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqyC_hsjkS0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# by句子去猜，用句號分隔\n",
        "from itertools import groupby\n",
        "from itertools import islice \n",
        "\n",
        "def split(word):\n",
        "    return [char for char in word]\n",
        "\n",
        "def BIO_encode(content, label):\n",
        "    \"\"\"\n",
        "        content: str\n",
        "        label: list of names\n",
        "    \"\"\"\n",
        "\n",
        "    if str(label) == \"['']\": \n",
        "\n",
        "      Context_list, IBO_list = split(content)+[''], np.repeat('O', len(content)+1).tolist()\n",
        "\n",
        "    else:\n",
        "\n",
        "      start_idx = []\n",
        "      end_idx = []\n",
        "      name_list = []\n",
        "      NER_list = []\n",
        "\n",
        "      for name in label:\n",
        "          # name = truth[0]\n",
        "          name_start = [m.start() for m in re.finditer(name, content)]\n",
        "          name_end = list(np.asarray(name_start) + len(name))\n",
        "\n",
        "          # 紀錄entity & 名字名稱\n",
        "          NER_list += np.repeat('PERSON', len(name_start)).tolist()\n",
        "          name_list += np.repeat(name, len(name_start)).tolist()\n",
        "          \n",
        "          # 名字在文章中的start, end\n",
        "          start_idx += name_start\n",
        "          end_idx += name_end\n",
        "\n",
        "      # 列出剛剛找到的entity在文章中的資訊\n",
        "      entity_info_list = [set(zip(start_idx, end_idx, NER_list, name_list))]\n",
        "\n",
        "      review_PER = []\n",
        "      for entity in sorted(entity_info_list[0]):\n",
        "          # print(entity)\n",
        "          if entity[2] == 'PERSON':\n",
        "              review_PER.append(entity[3])\n",
        "\n",
        "      Context_list=[]\n",
        "      IBO_list = []\n",
        "\n",
        "      sorted_entity_info_list = sorted(entity_info_list[0])\n",
        "\n",
        "      for i, entity_info in enumerate(sorted_entity_info_list):\n",
        "          # i = 0\n",
        "          # j = sorted(entity_info_list[0])[i]\n",
        "          # print(i, entity_info)\n",
        "          word_start_loc, word_end_loc, flag, word = entity_info\n",
        "\n",
        "          # 補O\n",
        "          if word_start_loc != 0:\n",
        "              if i != 0:\n",
        "                  O_start_loc = sorted(sorted_entity_info_list)[i-1][1]\n",
        "              else:\n",
        "                  O_start_loc = 0\n",
        "\n",
        "              O_end_loc = word_start_loc\n",
        "              add_O_string = content[O_start_loc:O_end_loc]\n",
        "              sub_split_words = split(add_O_string)\n",
        "              sub_IBO = np.repeat('O', len(add_O_string)).tolist()\n",
        "\n",
        "              Context_list += sub_split_words\n",
        "              IBO_list += sub_IBO\n",
        "          \n",
        "          # 當圈人名\n",
        "          sub_split_words = split(word)\n",
        "\n",
        "          # 人名\n",
        "          if (flag == 'PERSON') or (flag == 'ORG') or (flag == 'GPE'):\n",
        "\n",
        "              # 人名\n",
        "              if (flag == 'PERSON'):\n",
        "                  flag2 = 'PER'\n",
        "              # 機構名\n",
        "              if (flag == 'ORG'):\n",
        "                  flag2 = 'ORG'\n",
        "              # 地名\n",
        "              if (flag == 'GPE'):\n",
        "                  flag2 = 'LOC'\n",
        "\n",
        "              if len(word) > 1:\n",
        "                  sub_IBO = ['B-' + flag2] + np.repeat('I-' + flag2, len(word) - 1).tolist()\n",
        "              else:\n",
        "                  sub_IBO = ['B-' + flag2]\n",
        "          else:\n",
        "              flag2 = 'O'\n",
        "              sub_IBO = np.repeat(flag2, len(word)).tolist()\n",
        "          \n",
        "          Context_list += sub_split_words\n",
        "          IBO_list += sub_IBO\n",
        "\n",
        "          # 上面處理完，但最後一個人名後面還有一堆字需要補O\n",
        "          if word_end_loc != len(content) and i == (len(sorted_entity_info_list)-1):\n",
        "              word = content[word_end_loc:]\n",
        "              sub_split_words = split(word)\n",
        "              sub_IBO = np.repeat('O', len(word)).tolist()\n",
        "\n",
        "              Context_list += sub_split_words\n",
        "              IBO_list += sub_IBO\n",
        "\n",
        "      Context_list.append('')\n",
        "      IBO_list.append('')\n",
        "\n",
        "    return Context_list, IBO_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD5wX5_g1bMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context, ibo = BIO_encode(content=input_context, label=truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8OrDfEONf5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_context_under_max_seqLen(input_content):\n",
        "  split_new_content_list = []\n",
        "  for content in input_content:\n",
        "    if len(content)>=100:\n",
        "      sub_split_new_content_list = [content[x:x+99] for x in range(0, len(content), 99)]\n",
        "      for sub_split_content in sub_split_new_content_list:\n",
        "          split_new_content_list += [sub_split_content]\n",
        "    else:\n",
        "      split_new_content_list += [content]\n",
        "  return split_new_content_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG9jBZgHrTeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split(word):\n",
        "    return [char for char in word]\n",
        "\n",
        "# 整理BIO格式\n",
        "all_content = []\n",
        "all_BIO = []\n",
        "fail_index =[]\n",
        "wrong_index = []\n",
        "all_page_idx = []\n",
        "\n",
        "for k, url_info in news_urls.iterrows():\n",
        "    input_content = str(url_info['content'])\n",
        "    names = str(url_info['name']) # [] -> len=2\n",
        "    \n",
        "    # 先去掉人名為空 & 新聞爬不到的人 (0701改成全吃)\n",
        "    if (input_content != 'nan') and (len(input_content) > 2): # (names != 'nan') and (len(names) > 2) and\n",
        "\n",
        "        ground_truth = names.replace(\"[\",'').replace(\"]\",'').replace(\"'\",'').split(\",\")\n",
        "        Context_list, IBO_list = BIO_encode(content=input_content, label=ground_truth)\n",
        "        \n",
        "        # 用句號隔開\n",
        "        sentence_period = (list(g) for _, g in groupby(Context_list, key='。'.__ne__))\n",
        "        split_content_list = [a + b for a, b in zip(sentence_period, sentence_period)]\n",
        "        \n",
        "        # 整理成x, y\n",
        "        ibo_iter = iter(IBO_list)\n",
        "        split_IBO_list = [list(islice(ibo_iter, elem)) for elem in [len(i) for i in split_content_list]]\n",
        "        \n",
        "        \n",
        "        # 如果x跟y的長度不一樣 -> 有問題\n",
        "        if len([item for sublist in split_content_list for item in sublist]) != len([item for sublist in split_IBO_list for item in sublist]):\n",
        "            wrong_index.append(k)\n",
        "            print(f\"index {k} has wrong match.\")\n",
        "\n",
        "        # (0702) 若句子長度>100, 則切斷, <100則維持原貌\n",
        "        split_content_list = split_context_under_max_seqLen(input_content=split_content_list)\n",
        "        split_IBO_list = split_context_under_max_seqLen(input_content=split_IBO_list)\n",
        "\n",
        "        all_content += split_content_list\n",
        "        all_BIO += split_IBO_list\n",
        "        all_page_idx += [np.repeat(k, r).tolist() for r in map(len, split_content_list)]\n",
        "\n",
        "        if (len(Context_list)-1) == len(input_content):\n",
        "            # print(k, 'OK')\n",
        "            pass\n",
        "        else:\n",
        "            print(k, 'FAIL')\n",
        "            fail_index.append(k)\n",
        "    else:\n",
        "        # print(k, 'No Name', names)\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Yc4J25QJWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('raw_data/all_content_emptyname_maxLen_0628.pkl', 'wb') as f:\n",
        "  pickle.dump(all_content, f)\n",
        "with open('raw_data/all_BIO_emptyname_maxLen_0628.pkl', 'wb') as f:\n",
        "  pickle.dump(all_BIO, f)\n",
        "with open('raw_data/all_idx_emptyname_maxLen_0628.pkl', 'wb') as f:\n",
        "  pickle.dump(all_page_idx, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}