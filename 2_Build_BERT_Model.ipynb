{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_Build_BERT_Model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZyrKoOnbRuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kashgari==1.1.5\n",
        "# !pip install \"kashgari>=1.1,<2.0\"\n",
        "!pip install \"tensorflow>=1.14,<2.0\"\n",
        "!pip install tensorflow-gpu==1.15.0\n",
        "!pip install keras-applications==1.0.8\n",
        "!pip install keras==2.3.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ebyExWs4pai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR0Q3ahLspU9",
        "colab_type": "text"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczAvo6zUoVI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0dea8b9f-ebfa-47d2-b589-08147bc54045"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        " \n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "os.chdir(r\"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder\")\n",
        "# os.chdir(r\"/content/drive/My Drive/Colab Notebooks/t-brain_2020_NLP_team_share_folder\")\n",
        "\n",
        "\n",
        "import pickle\n",
        "with open('raw_data/all_content_emptyname_maxLen_0628.pkl', 'rb') as f:  # all_content_emptyname_maxLen_0802\n",
        "  all_content = pickle.load(f)\n",
        "with open('raw_data/all_BIO_emptyname_maxLen_0628.pkl', 'rb') as f: # all_BIO_emptyname_maxLen_0802\n",
        "  all_BIO = pickle.load(f)\n",
        "with open('raw_data/all_idx_emptyname_maxLen_0628.pkl', 'rb') as f: # all_idx_emptyname_maxLen_0802\n",
        "  all_page_idx = pickle.load(f)\n",
        "\n",
        "dataset = pd.DataFrame({'content':all_content, 'BIO':all_BIO, 'page_idx':[i[0] for i in all_page_idx], 'idx':all_page_idx})\n",
        "dataset[\"NameEmpty\"] = dataset[\"BIO\"].apply(lambda x: int(all(np.array(x) == \"O\")))\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>BIO</th>\n",
              "      <th>page_idx</th>\n",
              "      <th>idx</th>\n",
              "      <th>NameEmpty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[近, 年, 來, 投, 資, 市, 場, 波, 動, 越, 來, 越, 明, 顯, ,, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[專, 家, 表, 示, ,, 採, 用, 量, 化, 交, 易, 策, 略, 投, 資, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[大, 數, 據, 時, 代, 來, 臨, ,, 風, 行, 歐, 美, 5, 0, 年, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[舉, 辦, 「, 時, 間, 序, 列, 與, 量, 化, 交, 易, 研, 討, 會, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[越, 來, 越, 多, 的, 基, 金, 公, 司, 重, 視, 量, 化, 交, 易, ...</td>\n",
              "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
              "      <td>0</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  ... NameEmpty\n",
              "0  [近, 年, 來, 投, 資, 市, 場, 波, 動, 越, 來, 越, 明, 顯, ,, ...  ...         1\n",
              "1  [專, 家, 表, 示, ,, 採, 用, 量, 化, 交, 易, 策, 略, 投, 資, ...  ...         1\n",
              "2  [大, 數, 據, 時, 代, 來, 臨, ,, 風, 行, 歐, 美, 5, 0, 年, ...  ...         1\n",
              "3  [舉, 辦, 「, 時, 間, 序, 列, 與, 量, 化, 交, 易, 研, 討, 會, ...  ...         1\n",
              "4  [越, 來, 越, 多, 的, 基, 金, 公, 司, 重, 視, 量, 化, 交, 易, ...  ...         1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2izSNm8svQf",
        "colab_type": "text"
      },
      "source": [
        "## Get Train/Val/Test indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTQEPOoSPQHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_validate_test_split(input_df, train_percent=.6, validate_percent=.2, seed=None):\n",
        "    input_list = np.unique(input_df['page_idx']).tolist()\n",
        "    np.random.seed(seed)\n",
        "    perm = np.random.permutation(range(len(input_list))).tolist()\n",
        "    m = len(input_list)\n",
        "    train_end = int(train_percent * m)\n",
        "    validate_end = int(validate_percent * m) + train_end\n",
        "    train = list(input_list[i] for i in perm[:train_end])\n",
        "    validate = list(input_list[i] for i in perm[train_end:validate_end])\n",
        "    test = list(input_list[i] for i in perm[validate_end:])\n",
        "        \n",
        "    # -- train data --\n",
        "    train_data = [dataset[dataset['page_idx'].isin(train)]['content'].tolist(), dataset[dataset['page_idx'].isin(train)]['BIO'].tolist(), dataset[dataset['page_idx'].isin(train)]['idx'].tolist()]\n",
        "\n",
        "    # -- valid data --\n",
        "    valid_data = [dataset[dataset['page_idx'].isin(validate)]['content'].tolist(), dataset[dataset['page_idx'].isin(validate)]['BIO'].tolist(), dataset[dataset['page_idx'].isin(validate)]['idx'].tolist()]\n",
        "\n",
        "    # -- test data --\n",
        "    test_data = [dataset[dataset['page_idx'].isin(test)]['content'].tolist(),  dataset[dataset['page_idx'].isin(test)]['BIO'].tolist(), dataset[dataset['page_idx'].isin(test)]['idx'].tolist()]\n",
        "\n",
        "    return train_data, valid_data, test_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-t55XvWRNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3cf95902-2ed4-4692-e0ad-f0ad56a65edc"
      },
      "source": [
        "train_data, valid_data, test_data = train_validate_test_split(input_df=dataset, train_percent=.6, validate_percent=.2, seed=123)\n",
        "train_x, train_y, train_idx =  train_data[0], train_data[1], train_data[2]\n",
        "valid_x, valid_y, valid_idx =  valid_data[0], valid_data[1], valid_data[2]\n",
        "test_x, test_y, test_idx =  test_data[0], test_data[1], test_data[2]\n",
        "print(f\"train data count: {len(train_x)}\")\n",
        "print(f\"validate data count: {len(valid_x)}\")\n",
        "print(f\"test data count: {len(test_x)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data count: 66804\n",
            "validate data count: 20721\n",
            "test data count: 20217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYSbCk4qQCVg",
        "colab_type": "text"
      },
      "source": [
        "### Customized Model - Double LSTM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri3h5JteGFm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Dict, Any\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from kashgari.tasks.labeling.base_model import BaseLabelingModel\n",
        "from kashgari.layers import L\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from keras.optimizers import Adam, SGD\n",
        "\n",
        "\n",
        "from kashgari.corpus import SMP2018ECDTCorpus\n",
        "from tensorflow import keras\n",
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "from kashgari.tasks.labeling import BiLSTM_Model, CNN_LSTM_Model\n",
        "\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "class NonMasking(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.supports_masking = True\n",
        "        super(NonMasking, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        input_shape = input_shape\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "    def call(self, x, mask=None):\n",
        "        return x\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "class DoubleBLSTMModel(BaseLabelingModel):\n",
        "    \"\"\"Bidirectional LSTM Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_blstm1': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_blstm2': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dropout': {\n",
        "                'rate': 0.4\n",
        "            },\n",
        "            'layer_time_distributed': {},\n",
        "            'layer_activation': {\n",
        "                'activation': 'softmax'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        # Define your layers\n",
        "        layer_blstm1 = L.Bidirectional(L.LSTM(**config['layer_blstm1']),\n",
        "                                       name='layer_blstm1')\n",
        "        layer_blstm2 = L.Bidirectional(L.LSTM(**config['layer_blstm2']),\n",
        "                                       name='layer_blstm2')\n",
        "\n",
        "        layer_dropout = L.Dropout(**config['layer_dropout'],\n",
        "                                  name='layer_dropout')\n",
        "\n",
        "        layer_time_distributed = L.TimeDistributed(L.Dense(output_dim,\n",
        "                                                           **config['layer_time_distributed']),\n",
        "                                                   name='layer_time_distributed')\n",
        "        layer_activation = L.Activation(**config['layer_activation'])\n",
        "\n",
        "        # Define tensor flow\n",
        "        tensor = layer_blstm1(embed_model.output)\n",
        "        tensor = layer_blstm2(tensor)\n",
        "        tensor = layer_dropout(tensor)\n",
        "        tensor = layer_time_distributed(tensor)\n",
        "        output_tensor = layer_activation(tensor)\n",
        "\n",
        "        # Init model\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='model/logs', update_freq=1000)\n",
        "\n",
        "bert_embed = BERTEmbedding('code/chinese_L-12_H-768_A-12',\n",
        "                                task=kashgari.LABELING,\n",
        "                                sequence_length=100)\n",
        "\n",
        "model = DoubleBLSTMModel(bert_embed)\n",
        "# This step will build token dict, label dict and model structure\n",
        "model.build_model(train_x, train_y, valid_x, valid_y)\n",
        "# Compile model with custom optimizer, you can also customize loss and metrics.\n",
        "# optimizer = RAdam()\n",
        "# model.compile_model(optimizer=optimizer, loss=categorical_focal_loss(gamma=2.0, alpha=0.25))\n",
        "\n",
        "eval_callback = EvalCallBack(kash_model=model,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=1)\n",
        "\n",
        "model.compile_model()\n",
        "\n",
        "# Train model\n",
        "model.fit(train_x, train_y, valid_x, valid_y, batch_size=128, epochs=5, callbacks=[eval_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AykXaWt-AspB",
        "colab_type": "text"
      },
      "source": [
        "### Customized Model - LSTM CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94es5a7GAwx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Dict, Any\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from kashgari.tasks.labeling.base_model import BaseLabelingModel\n",
        "from kashgari.layers import L\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from keras.optimizers import Adam, SGD\n",
        "\n",
        "\n",
        "from kashgari.corpus import SMP2018ECDTCorpus\n",
        "from tensorflow import keras\n",
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "from kashgari.tasks.labeling import BiLSTM_Model, CNN_LSTM_Model\n",
        "\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level='DEBUG')\n",
        "\n",
        "# class NonMasking(Layer):\n",
        "#     def __init__(self, **kwargs):\n",
        "#         self.supports_masking = True\n",
        "#         super(NonMasking, self).__init__(**kwargs)\n",
        "#     def build(self, input_shape):\n",
        "#         input_shape = input_shape\n",
        "#     def compute_mask(self, input, input_mask=None):\n",
        "#         # do not pass the mask to the next layers\n",
        "#         return None\n",
        "#     def call(self, x, mask=None):\n",
        "#         return x\n",
        "#     def get_output_shape_for(self, input_shape):\n",
        "#         return input_shape\n",
        "\n",
        "\n",
        "class LSTM_CNN_Model(BaseLabelingModel):\n",
        "    \"\"\"Bidirectional LSTM Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_blstm1': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dropout': {\n",
        "                'rate': 0.4\n",
        "            },\n",
        "            'layer_conv1d':{\n",
        "                'kernel_size': 10,\n",
        "                'padding': 'same',\n",
        "            },\n",
        "            'layer_time_distributed': {},\n",
        "            'layer_activation': {\n",
        "                'activation': 'softmax'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        # Define your layers\n",
        "        layer_blstm1 = L.Bidirectional(L.LSTM(**config['layer_blstm1']),\n",
        "                                       name='layer_blstm1')\n",
        "\n",
        "        layer_dropout = L.Dropout(**config['layer_dropout'],\n",
        "                                  name='layer_dropout')\n",
        "\n",
        "        layer_conv1d = L.Conv1D(output_dim, **config['layer_conv1d'])\n",
        "        layer_gmp = L.GlobalMaxPooling1D()\n",
        "        layer_bn = L.BatchNormalization()\n",
        "        layer_activation = L.Activation(**config['layer_activation'])\n",
        "\n",
        "        # Define tensor flow\n",
        "        tensor = layer_blstm1(embed_model.output)\n",
        "        tensor = layer_dropout(tensor)\n",
        "        tensor = layer_conv1d(tensor)\n",
        "        # tensor = layer_gmp(tensor)\n",
        "        output_tensor = layer_activation(tensor)\n",
        "        # 最後維度 100 * 4\n",
        "\n",
        "        # Init model\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='model/logs', update_freq=1000)\n",
        "\n",
        "bert_embed = BERTEmbedding('code/chinese_L-12_H-768_A-12',\n",
        "                                task=kashgari.LABELING,\n",
        "                                sequence_length=100)\n",
        "\n",
        "model = LSTM_CNN_Model(bert_embed)\n",
        "# This step will build token dict, label dict and model structure\n",
        "model.build_model(train_x, train_y, valid_x, valid_y)\n",
        "# Compile model with custom optimizer, you can also customize loss and metrics.\n",
        "# optimizer = RAdam()\n",
        "# model.compile_model(optimizer=optimizer, loss=categorical_focal_loss(gamma=2.0, alpha=0.25))\n",
        "\n",
        "eval_callback = EvalCallBack(kash_model=model,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=1)\n",
        "\n",
        "model.compile_model()\n",
        "\n",
        "# Train model\n",
        "model.fit(train_x, train_y, valid_x, valid_y, batch_size=128, epochs=2, callbacks=[eval_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk21-pbgxpvQ",
        "colab_type": "text"
      },
      "source": [
        "### 用數據判斷判別器+關鍵字詞表的效果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiPVn30TxqVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import kashgari\n",
        "\n",
        "# load binary classifier\n",
        "model_binary_1 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_binary_1.h5')\n",
        "\n",
        "# keyword list\n",
        "keyword_list = ['吸金', '收賄', '洗錢','行賄','貪汙', '貪污','貪瀆','回扣', '賄賂', \"暴利\",'不法獲利', '詐欺', \"索賄\",\n",
        "                    '詐欺前科', '詐欺取財', '詐貸', '詐領', '詐騙', '詐取', '榨取', '暴力討債', '販毒', \"龐式騙局\", \"非法獲利\", \n",
        "                    '證交法', '證券交易法', '地下匯兌', '套利', '匯兌', '內線', \"捲款潛逃\", \"捲款\", '人頭戶', '仿冒品', '侵占', \n",
        "                    '偽造', '包庇', '弊案', '恐嚇取財', '掏空',  '海洛因', '炒股', '營業祕密法', '白手套', '竊盜', '期貨交易法', \n",
        "                    '經濟犯', '老鼠會', '製毒', '買票', '贓款', '走私', '逃漏', '逃漏稅', \"逃稅\", '銀行法', \"挪用\", \"弊端\", \n",
        "                    \"涉弊\", \"牟利\", \"浮報\", \"虛報\", \"黑金\", \"資恐\", \"恐怖主義\", \"廉政\", \"收受\", \"假帳\", \"地下錢莊\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBYR46Xp0Wps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_AML_model(AML_model):\n",
        "    global AML_classifier\n",
        "    AML_classifier = AML_model\n",
        "\n",
        "load_AML_model(AML_model=model_binary_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBR78FuozkJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AML_predict(article):\n",
        "    tensor = AML_classifier.embedding.process_x_dataset(data_preprocess(purge_char(article)))\n",
        "    probs = AML_classifier.tf_model.predict(tensor)\n",
        "    # print(\"probs\", probs)\n",
        "    if np.max(probs[:, 1]) < 0.3: \n",
        "        # print(\"Predict by AML Classifier and returned [].\")\n",
        "        answer = 0\n",
        "    else:\n",
        "        answer = 1\n",
        "    return answer\n",
        "\n",
        "def Keyword_predict(article):\n",
        "    answer = 0\n",
        "    for keyword in keyword_list:\n",
        "        if keyword in article:\n",
        "            answer = 1\n",
        "            break\n",
        "    return answer, keyword"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImtMq7-00yxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Data\n",
        "news_urls = pd.read_csv('raw_data/train_data_custom_0628.csv')\n",
        "\n",
        "question_list = []\n",
        "answer = []\n",
        "\n",
        "for _, news in news_urls.iterrows():\n",
        "    if \"省略內文\" not in news[\"content\"]:\n",
        "        question_list.append(news['content'])\n",
        "        answer.append(int(news['name']!=\"[]\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q3qQ2Jh0Mtf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "pred_results = {\"Article\":[], \"Classifier_pred\": [], \"Keyword_pred\": []}\n",
        "for i in range(len(question_list)):\n",
        "    \n",
        "    pred_results[\"Article\"].append(question_list[i])\n",
        "    pred_results[\"Classifier_pred\"].append(AML_predict(question_list[i]))\n",
        "    pred_results[\"Keyword_pred\"].append(Keyword_predict(question_list[i])[0])\n",
        "\n",
        "    print(\"No: \", i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eOK7HMz42yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_results[\"Ground_truth\"] = answer\n",
        "result = pd.DataFrame(pred_results)\n",
        "result.to_excel(\"compare/AML_classifier_and_keyword_list_SCORE.xlsx\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYzMoiXx6Qk4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c90286b1-1d1b-4e9f-a418-730447cfd7d9"
      },
      "source": [
        "# 計算F1 Score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "result[\"Classifier_with_keyword\"] = ((result[\"Classifier_pred\"] == 1) & (result[\"Keyword_pred\"] == 1)).astype(int)\n",
        "\n",
        "# \n",
        "print(\"Classifier_with_keyword: \", f1_score(result[\"Ground_truth\"], result[\"Classifier_with_keyword\"]))\n",
        "print(\"Classifier_pred: \", f1_score(result[\"Ground_truth\"], result[\"Classifier_pred\"]))\n",
        "print(\"Keyword_pred: \", f1_score(result[\"Ground_truth\"], result[\"Keyword_pred\"]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier_with_keyword:  0.9278350515463918\n",
            "Classifier_pred:  0.7288888888888888\n",
            "Keyword_pred:  0.7464454976303317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7F_eo-CSgae",
        "colab_type": "text"
      },
      "source": [
        "### 判斷AML文章的Binary Clssifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOuh4ButThYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "AML_y = [1 if \"B-PER\" in y else 0 for y in train_y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_w5id00oFEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding: utf-8\n",
        "\n",
        "# author: BrikerMan\n",
        "# contact: eliyar917@gmail.com\n",
        "# blog: https://eliyar.biz\n",
        "\n",
        "# file: models.py\n",
        "# time: 2019-05-20 11:13\n",
        "\n",
        "import logging\n",
        "from typing import Dict, Any\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "from kashgari.tasks.labeling.base_model import BaseLabelingModel\n",
        "from kashgari.layers import L\n",
        "from kashgari.layers.crf import CRF\n",
        "\n",
        "from kashgari import custom_objects\n",
        "\n",
        "custom_objects['CRF'] = CRF\n",
        "\n",
        "\n",
        "class BiLSTM_Model(BaseLabelingModel):\n",
        "    \"\"\"Bidirectional LSTM Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_blstm': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dropout': {\n",
        "                'rate': 0.4\n",
        "            },\n",
        "            'layer_time_distributed': {},\n",
        "            'layer_activation': {\n",
        "                'activation': 'softmax'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        layer_blstm = L.Bidirectional(L.LSTM(**config['layer_blstm']),\n",
        "                                      name='layer_blstm')\n",
        "\n",
        "        layer_dropout = L.Dropout(**config['layer_dropout'],\n",
        "                                  name='layer_dropout')\n",
        "\n",
        "        layer_time_distributed = L.TimeDistributed(L.Dense(output_dim,\n",
        "                                                           **config['layer_time_distributed']),\n",
        "                                                   name='layer_time_distributed')\n",
        "        layer_activation = L.Activation(**config['layer_activation'])\n",
        "\n",
        "        tensor = layer_blstm(embed_model.output)\n",
        "        tensor = layer_dropout(tensor)\n",
        "        tensor = layer_time_distributed(tensor)\n",
        "        output_tensor = layer_activation(tensor)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "\n",
        "class BiLSTM_CRF_Model(BaseLabelingModel):\n",
        "    \"\"\"Bidirectional LSTM CRF Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_blstm': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dense': {\n",
        "                'units': 64,\n",
        "                'activation': 'tanh'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        layer_blstm = L.Bidirectional(L.LSTM(**config['layer_blstm']),\n",
        "                                      name='layer_blstm')\n",
        "\n",
        "        layer_dense = L.Dense(**config['layer_dense'], name='layer_dense')\n",
        "        layer_crf_dense = L.Dense(output_dim, name='layer_crf_dense')\n",
        "        layer_crf = CRF(output_dim, name='layer_crf')\n",
        "\n",
        "        tensor = layer_blstm(embed_model.output)\n",
        "        tensor = layer_dense(tensor)\n",
        "        tensor = layer_crf_dense(tensor)\n",
        "        output_tensor = layer_crf(tensor)\n",
        "\n",
        "        self.layer_crf = layer_crf\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "    def compile_model(self, **kwargs):\n",
        "        if kwargs.get('loss') is None:\n",
        "            kwargs['loss'] = self.layer_crf.loss\n",
        "        if kwargs.get('metrics') is None:\n",
        "            kwargs['metrics'] = [self.layer_crf.viterbi_accuracy]\n",
        "        super(BiLSTM_CRF_Model, self).compile_model(**kwargs)\n",
        "\n",
        "\n",
        "class BiGRU_Model(BaseLabelingModel):\n",
        "    \"\"\"Bidirectional GRU Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_bgru': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dropout': {\n",
        "                'rate': 0.4\n",
        "            },\n",
        "            'layer_time_distributed': {},\n",
        "            'layer_activation': {\n",
        "                'activation': 'softmax'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        layer_blstm = L.Bidirectional(L.GRU(**config['layer_bgru']),\n",
        "                                      name='layer_bgru')\n",
        "\n",
        "        layer_dropout = L.Dropout(**config['layer_dropout'],\n",
        "                                  name='layer_dropout')\n",
        "\n",
        "        layer_time_distributed = L.TimeDistributed(L.Dense(output_dim,\n",
        "                                                           **config['layer_time_distributed']),\n",
        "                                                   name='layer_time_distributed')\n",
        "        layer_activation = L.Activation(**config['layer_activation'])\n",
        "\n",
        "        tensor = layer_blstm(embed_model.output)\n",
        "        tensor = layer_dropout(tensor)\n",
        "        tensor = layer_time_distributed(tensor)\n",
        "        output_tensor = layer_activation(tensor)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "\n",
        "class BiGRU_CRF_Model(BaseLabelingModel):\n",
        "    \"\"\"Bidirectional GRU CRF Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_bgru': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dense': {\n",
        "                'units': 64,\n",
        "                'activation': 'tanh'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        layer_blstm = L.Bidirectional(L.GRU(**config['layer_bgru']),\n",
        "                                      name='layer_bgru')\n",
        "\n",
        "        layer_dense = L.Dense(**config['layer_dense'], name='layer_dense')\n",
        "        layer_crf_dense = L.Dense(output_dim, name='layer_crf_dense')\n",
        "        layer_crf = CRF(output_dim, name='layer_crf')\n",
        "\n",
        "        tensor = layer_blstm(embed_model.output)\n",
        "        tensor = layer_dense(tensor)\n",
        "        tensor = layer_crf_dense(tensor)\n",
        "        output_tensor = layer_crf(tensor)\n",
        "\n",
        "        self.layer_crf = layer_crf\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "    def compile_model(self, **kwargs):\n",
        "        if kwargs.get('loss') is None:\n",
        "            kwargs['loss'] = self.layer_crf.loss\n",
        "        if kwargs.get('metrics') is None:\n",
        "            kwargs['metrics'] = [self.layer_crf.viterbi_accuracy]\n",
        "        super(BiGRU_CRF_Model, self).compile_model(**kwargs)\n",
        "\n",
        "\n",
        "class CNN_LSTM_Model(BaseLabelingModel):\n",
        "    \"\"\"CNN LSTM Sequence Labeling Model\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Get hyper parameters of model\n",
        "        Returns:\n",
        "            hyper parameters dict\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'layer_conv': {\n",
        "                'filters': 32,\n",
        "                'kernel_size': 3,\n",
        "                'padding': 'same',\n",
        "                'activation': 'relu'\n",
        "            },\n",
        "            'layer_lstm': {\n",
        "                'units': 128,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'layer_dropout': {\n",
        "                'rate': 0.4\n",
        "            },\n",
        "            'layer_time_distributed': {},\n",
        "            'layer_activation': {\n",
        "                'activation': 'softmax'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        \"\"\"\n",
        "        build model architectural\n",
        "        \"\"\"\n",
        "        output_dim = len(self.processor.label2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        layer_conv = L.Conv1D(**config['layer_conv'],\n",
        "                              name='layer_conv')\n",
        "        layer_lstm = L.LSTM(**config['layer_lstm'],\n",
        "                            name='layer_lstm')\n",
        "        layer_dropout = L.Dropout(**config['layer_dropout'],\n",
        "                                  name='layer_dropout')\n",
        "        layer_time_distributed = L.TimeDistributed(L.Dense(output_dim,\n",
        "                                                           **config['layer_time_distributed']),\n",
        "                                                   name='layer_time_distributed')\n",
        "        layer_activation = L.Activation(**config['layer_activation'])\n",
        "\n",
        "        tensor = layer_conv(embed_model.output)\n",
        "        tensor = layer_lstm(tensor)\n",
        "        tensor = layer_dropout(tensor)\n",
        "        tensor = layer_time_distributed(tensor)\n",
        "        output_tensor = layer_activation(tensor)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, output_tensor)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.DEBUG)\n",
        "    from kashgari.corpus import ChineseDailyNerCorpus\n",
        "\n",
        "    valid_x, valid_y = ChineseDailyNerCorpus.load_data('train')\n",
        "\n",
        "    model = BiLSTM_CRF_Model()\n",
        "    model.fit(valid_x, valid_y, epochs=50, batch_size=64)\n",
        "    model.evaluate(valid_x, valid_y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRno8YWQt21s",
        "colab_type": "text"
      },
      "source": [
        "### 套件CRF 實作"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvqCv3jgt10N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encoding: utf-8\n",
        "\n",
        "# author: BrikerMan\n",
        "# contact: eliyar917@gmail.com\n",
        "# blog: https://eliyar.biz\n",
        "\n",
        "# file: crf.py\n",
        "# time: 2019-06-28 14:33\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class CRF(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        Conditional Random Field layer (tf.keras)\n",
        "        `CRF` can be used as the last layer in a network (as a classifier). Input shape (features)\n",
        "        must be equal to the number of classes the CRF can predict (a linear layer is recommended).\n",
        "        Note: the loss and accuracy functions of networks using `CRF` must\n",
        "        use the provided loss and accuracy functions (denoted as loss and viterbi_accuracy)\n",
        "        as the classification of sequences are used with the layers internal weights.\n",
        "        Args:\n",
        "            output_dim (int): the number of labels to tag each temporal input.\n",
        "        Input shape:\n",
        "            nD tensor with shape `(batch_size, sentence length, num_classes)`.\n",
        "        Output shape:\n",
        "            nD tensor with shape: `(batch_size, sentence length, num_classes)`.\n",
        "        \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_dim,\n",
        "                 mode='reg',\n",
        "                 supports_masking=False,\n",
        "                 transitions=None,\n",
        "                 **kwargs):\n",
        "        self.transitions = None\n",
        "        super(CRF, self).__init__(**kwargs)\n",
        "        self.output_dim = int(output_dim)\n",
        "        self.mode = mode\n",
        "        if self.mode == 'pad':\n",
        "            self.input_spec = [tf.keras.layers.InputSpec(min_ndim=3), tf.keras.layers.InputSpec(min_ndim=2)]\n",
        "        elif self.mode == 'reg':\n",
        "            self.input_spec = tf.keras.layers.InputSpec(min_ndim=3)\n",
        "        else:\n",
        "            raise ValueError\n",
        "        self.supports_masking = supports_masking\n",
        "        self.sequence_lengths = None\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'mode': self.mode,\n",
        "            'supports_masking': self.supports_masking,\n",
        "            'transitions': tf.keras.backend.eval(self.transitions)\n",
        "        }\n",
        "        base_config = super(CRF, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.mode == 'pad':\n",
        "            assert len(input_shape) == 2\n",
        "            assert len(input_shape[0]) == 3\n",
        "            assert len(input_shape[1]) == 2\n",
        "            f_shape = tf.TensorShape(input_shape[0])\n",
        "            input_spec = [tf.keras.layers.InputSpec(min_ndim=3, axes={-1: f_shape[-1]}),\n",
        "                          tf.keras.layers.InputSpec(min_ndim=2, axes={-1: 1}, dtype=tf.int32)]\n",
        "        else:\n",
        "            assert len(input_shape) == 3\n",
        "            f_shape = tf.TensorShape(input_shape)\n",
        "            input_spec = tf.keras.layers.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
        "\n",
        "        if f_shape[-1] is None:\n",
        "            raise ValueError('The last dimension of the inputs to `CRF` should be defined. Found `None`.')\n",
        "        if f_shape[-1] != self.output_dim:\n",
        "            raise ValueError('The last dimension of the input shape must be equal to output shape. '\n",
        "                             'Use a linear layer if needed.')\n",
        "        self.input_spec = input_spec\n",
        "        self.transitions = self.add_weight(name='transitions',\n",
        "                                           shape=[self.output_dim, self.output_dim],\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.mode == 'pad':\n",
        "            sequences = tf.convert_to_tensor(inputs[0], dtype=self.dtype)\n",
        "            self.sequence_lengths = tf.keras.backend.flatten(inputs[-1])\n",
        "        else:\n",
        "            sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "            shape = tf.shape(inputs)\n",
        "            self.sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
        "        viterbi_sequence, _ = tf.contrib.crf.crf_decode(sequences, self.transitions,\n",
        "                                        self.sequence_lengths)\n",
        "        output = tf.keras.backend.one_hot(viterbi_sequence, self.output_dim)\n",
        "        return tf.keras.backend.in_train_phase(sequences, output)\n",
        "\n",
        "    def loss(self, y_true, y_pred):\n",
        "        y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
        "        log_likelihood, self.transitions = tf.contrib.crf.crf_log_likelihood(y_pred,\n",
        "                                            tf.cast(tf.keras.backend.argmax(y_true),\n",
        "                                                    dtype=tf.int32),\n",
        "                                            self.sequence_lengths,\n",
        "                                            transition_params=self.transitions)\n",
        "        return tf.reduce_mean(-log_likelihood)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.mode == 'pad':\n",
        "            data_shape = input_shape[0]\n",
        "        else:\n",
        "            data_shape = input_shape\n",
        "        tf.TensorShape(data_shape).assert_has_rank(3)\n",
        "        return data_shape[:2] + (self.output_dim,)\n",
        "\n",
        "    @property\n",
        "    def viterbi_accuracy(self):\n",
        "        def accuracy(y_true, y_pred):\n",
        "            shape = tf.shape(y_pred)\n",
        "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
        "            viterbi_sequence, _ = tf.contrib.crf.crf_decode(y_pred, self.transitions, sequence_lengths)\n",
        "            output = tf.keras.backend.one_hot(viterbi_sequence, self.output_dim)\n",
        "            return tf.keras.metrics.categorical_accuracy(y_true, output)\n",
        "\n",
        "        accuracy.func_name = 'viterbi_accuracy'\n",
        "        return accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khgVx0U5P-4e",
        "colab_type": "text"
      },
      "source": [
        "### Example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrvgYAk5lVBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "453c43db-ccfe-4a82-b43a-ada69790b6b2"
      },
      "source": [
        "from tensorflow import keras\n",
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "from kashgari.tasks.labeling import BiLSTM_Model\n",
        "\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "\n",
        "from kashgari.tasks.labeling.abc_model import ABCLabelingModel\n",
        "from kashgari.layers import L\n",
        "\n",
        "\n",
        "tf_board_callback = keras.callbacks.TensorBoard(log_dir='model/logs', update_freq=1000)\n",
        "\n",
        "bert_embed = BERTEmbedding('code/chinese_L-12_H-768_A-12',\n",
        "                           task=kashgari.LABELING,\n",
        "                           sequence_length=100)\n",
        "\n",
        "model = BiLSTM_Model(bert_embed)\n",
        "# Build-in callback for print precision, recall and f1 at every epoch step\n",
        "eval_callback = EvalCallBack(kash_model=model,\n",
        "                             valid_x=valid_x,\n",
        "                             valid_y=valid_y,\n",
        "                             step=5)\n",
        "model.fit(x_train=train_x,\n",
        "          y_train=train_y,\n",
        "          x_validate=valid_x,\n",
        "          y_validate=valid_y,\n",
        "          epochs=5,\n",
        "          batch_size=128,\n",
        "          callbacks=[eval_callback, tf_board_callback])\n",
        "\n",
        "# 輸出到tensorboard上\n",
        "!tensorboard --logdir='model/logs/' \n",
        "\n",
        "model.evaluate(test_x, test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "      PER     0.7293    0.9034    0.8070       507\n",
            "\n",
            "micro avg     0.7293    0.9034    0.8070       507\n",
            "macro avg     0.7293    0.9034    0.8070       507\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'           precision    recall  f1-score   support\\n\\n      PER     0.7293    0.9034    0.8070       507\\n\\nmicro avg     0.7293    0.9034    0.8070       507\\nmacro avg     0.7293    0.9034    0.8070       507\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq2c0wubZOY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81761afe-7b93-4ee9-e4a2-c58cd1debc66"
      },
      "source": [
        "from kashgari.corpus import SMP2018ECDTCorpus\n",
        "from kashgari.tasks.classification import BiLSTM_Model\n",
        "\n",
        "train_x_demo, train_y_demo = SMP2018ECDTCorpus.load_data('train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://s3.bmio.net/kashgari/SMP2018ECDTCorpus.tar.gz\n",
            "49152/46021 [================================] - 0s 9us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGBCVETqtLNp",
        "colab_type": "text"
      },
      "source": [
        "Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdOTWQJYWVex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def categorical_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Softmax version of focal loss.\n",
        "           m\n",
        "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
        "          c=1\n",
        "      where m = number of classes, c = class and o = observation\n",
        "    Parameters:\n",
        "      alpha -- the same as weighing factor in balanced cross entropy\n",
        "      gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "      gamma -- 2.0 as mentioned in the paper\n",
        "      alpha -- 0.25 as mentioned in the paper\n",
        "    References:\n",
        "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
        "    Usage:\n",
        "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred: A tensor resulting from a softmax\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        # print(K.shape(y_pred))\n",
        "        # Scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "        # Clip the prediction value to prevent NaN's and Inf's\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        # Calculate Cross Entropy\n",
        "        cross_entropy = -tf.multiply(y_true, K.log(y_pred))\n",
        "\n",
        "        # Calculate Focal Loss\n",
        "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "        # Compute mean loss in mini_batch\n",
        "        return K.mean(loss, axis=1)\n",
        "\n",
        "    return categorical_focal_loss_fixed\n",
        "\n",
        "# from kashgari.tasks.labeling import BiLSTM_Model\n",
        "# model = BiLSTM_Model(bert_embed)\n",
        "\n",
        "# 可用focal loss?\n",
        "\n",
        "# model.compile_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIbk8ejwnuYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def multi_category_focal_loss2(gamma=2., alpha=.25):\n",
        "    epsilon = 1.e-7\n",
        "    gamma = float(gamma)\n",
        "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
        "\n",
        "    def multi_category_focal_loss2_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
        "    \n",
        "        alpha_t = y_true*alpha + (tf.ones_like(y_true)-y_true)*(1-alpha)\n",
        "        y_t = tf.multiply(y_true, y_pred) + tf.multiply(1-y_true, 1-y_pred)\n",
        "        ce = -tf.log(y_t)\n",
        "        weight = tf.pow(tf.subtract(1., y_t), gamma)\n",
        "        fl = tf.multiply(tf.multiply(weight, ce), alpha_t)\n",
        "        loss = tf.reduce_mean(fl)\n",
        "        return loss\n",
        "    return multi_category_focal_loss2_fixed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y01uzM703PTM",
        "colab_type": "text"
      },
      "source": [
        "### BiLSTM CRF\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLKGy4vjEqpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from kashgari.corpus import SMP2018ECDTCorpus\n",
        "from tensorflow import keras\n",
        "import kashgari\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "from kashgari.tasks.labeling import BiLSTM_Model, CNN_LSTM_Model\n",
        "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "from kashgari.callbacks import EvalCallBack\n",
        "# Remember to import kashgari before than RAdam\n",
        "# from keras_radam import RAdam\n",
        "\n",
        "bert_embed = BERTEmbedding('code/chinese_L-12_H-768_A-12',\n",
        "                           task=kashgari.LABELING,\n",
        "                           sequence_length=100)\n",
        "\n",
        "model = BiLSTM_CRF_Model(bert_embed) \n",
        "# # This step will build token dict, label dict and model structure\n",
        "model.build_model(train_x, train_y) # , valid_x, valid_y\n",
        "# # Compile model with custom optimizer, you can also customize loss and metrics.\n",
        "# # optimizer = RAdam()\n",
        "# # model.compile_model(optimizer=optimizer, loss=categorical_focal_loss(gamma=2.0, alpha=0.25))\n",
        "\n",
        "# eval_callback = EvalCallBack(kash_model=model,\n",
        "#                              valid_x=valid_x,\n",
        "#                              valid_y=valid_y,\n",
        "#                              step=1)\n",
        "\n",
        "model.compile_model(optimizer='rmsprop')\n",
        "\n",
        "# Train model\n",
        "model.fit(train_x, train_y, batch_size=128, epochs=7) # , callbacks=[eval_callback] , valid_x, valid_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o59MZFXeyY_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"model/model_all_r_6.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpWtE1njcYds",
        "colab_type": "text"
      },
      "source": [
        "### 比賽output格式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvW_1aMucS9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "d911b7df-87cc-4f35-f478-8d9716b0e35f"
      },
      "source": [
        "import kashgari\n",
        "\n",
        "model_binary_1 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_binary_1.h5')\n",
        "# model_binary_2 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_binary_2.h5')\n",
        "\n",
        "# model_J_40 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_J_40.h5')\n",
        "# model_all_r_1 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_r_1.h5')\n",
        "model_all_r_2 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_r_2.h5')\n",
        "# model_all_r_3 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_r_3.h5')\n",
        "# model_all_r_4 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_r_4.h5')\n",
        "# model_all_r_5 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_r_5.h5')\n",
        "model_all_r_6 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_r_6.h5')\n",
        "\n",
        "# model_all_J_1 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_J_1.h5')\n",
        "# model_all_J_2 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_J_2.h5')\n",
        "# model_all_J_3 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_J_3.h5')\n",
        "# model_all_J_4 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_all_J_4.h5')\n",
        "# model_1_09 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_1.09.h5')\n",
        "# model_21 = kashgari.utils.load_model('/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/model/model_21.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n",
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:22: UserWarning: bert4keras.bert has been renamed as bert4keras.models.\n",
            "  warnings.warn('bert4keras.bert has been renamed as bert4keras.models.')\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:23: UserWarning: please use bert4keras.models.\n",
            "  warnings.warn('please use bert4keras.models.')\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:44: UserWarning: bert4keras.tokenizer has been renamed as bert4keras.tokenizers.\n",
            "  warnings.warn('bert4keras.tokenizer has been renamed as bert4keras.tokenizers.')\n",
            "/usr/local/lib/python3.6/dist-packages/bert4keras/__init__.py:45: UserWarning: please use bert4keras.tokenizers.\n",
            "  warnings.warn('please use bert4keras.tokenizers.')\n",
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n",
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gT6DnY-TEL5",
        "colab_type": "text"
      },
      "source": [
        "### 測試賽題目"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgmurJX8TD0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "file_root = \"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/questions/0805_questions\"\n",
        "file_dirs = os.listdir(r\"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/questions/0805_questions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqowKtSBVncx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "question_list = [pd.read_pickle(os.path.join(file_root, file_dir)) for file_dir in file_dirs if file_dir.startswith(\"Q\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDDOgUWY5gBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# question_dict = {}\n",
        "# question_dict[\"content\"] = question_list\n",
        "# data = pd.DataFrame(question_dict)\n",
        "# data.to_csv(\"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/questions/0729_question_content_df.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hYp6NDdXcTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [i for i, q in enumerate(question_list) if len(q) == 4653]\n",
        "# [i for i, q in enumerate(question_list) if len(q) > 3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t5S53z3ptMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "998f7055-bf71-49fc-cb05-92904db25ae1"
      },
      "source": [
        "idices = [i for i, q in enumerate(question_list) if len(q) > 3000]\n",
        "[(\"idx:\"+str(idx), \"len:\"+str(len(question_list[idx]))) for idx in idices]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('idx:19', 'len:23285'),\n",
              " ('idx:31', 'len:3068'),\n",
              " ('idx:42', 'len:3154'),\n",
              " ('idx:73', 'len:4380'),\n",
              " ('idx:161', 'len:4199'),\n",
              " ('idx:191', 'len:4108'),\n",
              " ('idx:263', 'len:7560'),\n",
              " ('idx:288', 'len:4377'),\n",
              " ('idx:312', 'len:3374'),\n",
              " ('idx:326', 'len:4315'),\n",
              " ('idx:351', 'len:3325'),\n",
              " ('idx:352', 'len:3368')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdF6TKKgMACp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 找出超時文章來測試\n",
        "article_idx = [i for i, q in enumerate(question_list) if len(q) > 3000]\n",
        "articles = [strQ2B(question_list[i]) for i in article_idx]\n",
        "testing_articles = pd.DataFrame({\"Test_Article\": articles})\n",
        "testing_articles.to_excel(\"raw_data/0802_timeout_article_test.xlsx\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0biXYYyML6wz",
        "colab_type": "text"
      },
      "source": [
        "### 正式賽題目檢討"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNAF5G2i7HOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "def pickle_dump(obj, file_name, output_dir=\"questions\"):\n",
        "    with open(os.path.join(output_dir, file_name), 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "\n",
        "def pickle_load(file_name, input_dir):\n",
        "    with open(os.path.join(input_dir, file_name), 'rb') as f:\n",
        "        s = pickle.load(f)\n",
        "    return s\n",
        "\n",
        "\n",
        "def split2char(word):\n",
        "    return [char for char in word]\n",
        "\n",
        "\n",
        "def split_context_under_max_seqLen(input_content):\n",
        "    split_new_content_list = []\n",
        "    for content in input_content:\n",
        "        if len(content) >= 100:\n",
        "            sub_split_new_content_list = [content[x:x+99] for x in range(0, len(content), 99)]\n",
        "            for sub_split_content in sub_split_new_content_list:\n",
        "                split_new_content_list += [sub_split_content]\n",
        "        else:\n",
        "            split_new_content_list += [content]\n",
        "    return split_new_content_list\n",
        "\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    if isinstance(ustring, str) :\n",
        "      ss = []\n",
        "      for s in ustring:\n",
        "          rstring = \"\"\n",
        "          for uchar in s:\n",
        "              inside_code = ord(uchar)\n",
        "              if inside_code == 12288: \n",
        "                  inside_code = 32\n",
        "              elif (inside_code >= 65281 and inside_code <= 65374):\n",
        "                  inside_code -= 65248\n",
        "              rstring += chr(inside_code)\n",
        "          ss.append(rstring)\n",
        "      return ''.join(ss)\n",
        "    else:\n",
        "      return unstring\n",
        "\n",
        "\n",
        "def strClean(x):\n",
        "    if isinstance(x, str):\n",
        "        x = re.sub(\"[▲◆▪【】*';％%※★<>#〈〉' '_|｜()（）－-]\",  \"\", strQ2B(x))\n",
        "        x = re.sub(\"[!！？?]\", \"。\", x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def purge_char(x, char_length=10000):\n",
        "    return x[:char_length]\n",
        "\n",
        "\n",
        "def load_model(model_, AML_model):\n",
        "    # ref1: https://blog.techbridge.cc/2018/11/01/python-flask-keras-image-predict-api/\n",
        "    # ref1.1: https://stackoverflow.com/questions/47115946/tensor-is-not-an-element-of-this-graph\n",
        "    # ref2: https://github.com/tensorflow/tensorflow/issues/28287\n",
        "    global sess\n",
        "    global graph\n",
        "    global model\n",
        "    global AML_classifier\n",
        "\n",
        "    # initialize tf graph\n",
        "    # sess = tf.Session()\n",
        "    # graph = tf.get_default_graph()\n",
        "    # load model\n",
        "    # set_session(sess)\n",
        "    # model = kashgari.utils.load_model('model_J_40.h5')\n",
        "    # AML_classifier = kashgari.utils.load_model('model_binary_1.h5')\n",
        "    model = model_\n",
        "    AML_classifier = AML_model\n",
        "\n",
        "def data_preprocess(data):\n",
        "    return split_context_under_max_seqLen([split2char(strClean(data))])\n",
        "\n",
        "def load_tabu_list():\n",
        "    global tabu_list\n",
        "\n",
        "    tabu_list = [\"佰萬\", \"仟萬\", \"百萬\", \"千萬\", \"一億\", \"兩億\", \"三億\", \"四億\", \"五億\", \"六億\", \"七億\", \"八億\", \"九億\", \"十億\",\n",
        "                 \"參億\", \"肆億\", \"伍億\", \"陸億\", \"柒億\", \"捌億\", \"玖億\", \"拾億\", \"壹億\", \"貳億\",\n",
        "                 \"二億\", \"億萬\", \"兆元\", \"億元\", \"千元\", \"萬元\", \"百元\", \"一審\", \"二審\", \"三審\"]\n",
        "\n",
        "\n",
        "def load_keyword_list():\n",
        "    global keyword_list\n",
        "\n",
        "    keyword_list = ['吸金', '收賄', '洗錢','行賄','貪汙', '貪污','貪瀆','回扣', '賄賂', \"暴利\",'不法獲利', '詐欺',\n",
        "                    '詐欺前科', '詐欺取財', '詐貸', '詐領', '詐騙', '詐取', '榨取', '暴力討債', '毒品', '販毒', \n",
        "                    '證交法', '證券交易法', '地下匯兌', '套利', '匯兌', '內線', \"捲款潛逃\", \"捲款\", '人頭戶', '仿冒品', '侵占', \n",
        "                    '偽造', '包庇', '弊案', '恐嚇取財', '掏空',  '海洛因', '炒股', '營業祕密法', '白手套', '竊盜', '期貨交易法', \n",
        "                    '經濟犯', '老鼠會', '製毒', '買票', '贓款', '走私', '逃漏', '逃漏稅', \"逃稅\", '銀行法', \"挪用\", \"弊端\", \n",
        "                    \"涉弊\", \"牟利\", \"浮報\", \"虛報\", \"黑金\", \"資恐\", \"恐怖主義\", \"廉政\", \"收受\", \"假帳\", \"地下錢莊\"]\n",
        "\n",
        "def keyword_classifier(article):\n",
        "    AML = False\n",
        "    for keyword in keyword_list:\n",
        "        if keyword in article:\n",
        "            AML = True\n",
        "            break\n",
        "    return AML\n",
        "\n",
        "\n",
        "def generate_server_uuid(input_string):\n",
        "    \"\"\" Create your own server_uuid\n",
        "    @param input_string (str): information to be encoded as server_uuid\n",
        "    @returns server_uuid (str): your unique server_uuid\n",
        "    \"\"\"\n",
        "    s = hashlib.sha256()\n",
        "    data = (input_string+SALT).encode(\"utf-8\")\n",
        "    s.update(data)\n",
        "    server_uuid = s.hexdigest()\n",
        "    return server_uuid\n",
        "\n",
        "\n",
        "def predict(news, threshold, keyword):\n",
        "    \"\"\" Predict your model result\n",
        "    @param article (str): a news article\n",
        "    @returns prediction (list): a list of name\n",
        "    \"\"\"\n",
        "\n",
        "    ####### PUT YOUR MODEL INFERENCING CODE HERE #######\n",
        "    # prediction = ['aha','danny','jack']\n",
        "\n",
        "    raw_text = data_preprocess(purge_char(news, char_length=7000))\n",
        "\n",
        "    # if not larger than threshold, return []\n",
        "    tensor = AML_classifier.embedding.process_x_dataset(raw_text)\n",
        "    probs = AML_classifier.tf_model.predict(tensor)\n",
        "\n",
        "    if np.max(probs[:, 1]) < threshold:\n",
        "        # print(\"Predict by AML Classifier and returned [].\")\n",
        "        return []\n",
        "    \n",
        "    if keyword:\n",
        "        if not keyword_classifier(news):\n",
        "            # print(\"Predict by Keyword Classifier and returned [].\")\n",
        "            return []\n",
        "    \n",
        "    raw_text = data_preprocess(purge_char(news, char_length=3200))\n",
        "    ners = model.predict(raw_text)\n",
        "\n",
        "    pred_names = []\n",
        "\n",
        "    for sect_idx in range(len(raw_text)):\n",
        "        ner_reg_list = []\n",
        "\n",
        "        for index, (word, tag) in enumerate(zip(raw_text[sect_idx], ners[sect_idx])):\n",
        "            if tag != 'O':\n",
        "                ner_reg_list.append((word, tag, index))\n",
        "\n",
        "        if ner_reg_list:\n",
        "            for i, item in enumerate(ner_reg_list):\n",
        "                if item[1].startswith('B'):\n",
        "                    label = \"\"\n",
        "                    end = i + 1\n",
        "                    while end <= len(ner_reg_list) - 1 and ner_reg_list[end][1].startswith('I') and (ner_reg_list[end][2] - ner_reg_list[end-1][2]) == 1: \n",
        "                        end += 1\n",
        "                    \n",
        "                    label += ''.join([item[0] for item in ner_reg_list[i:end]])\n",
        "                    pred_names.append(label)\n",
        "    \n",
        "    # output\n",
        "    if len(pred_names) > 0:\n",
        "        pred_names = np.unique([item.replace(' ','') for item in pred_names]).tolist()\n",
        "        pred_names = [name for name in pred_names if len(name) > 1]\n",
        "        # pred_names = _check_datatype_to_list(pred_names) # Officially provided\n",
        "\n",
        "        # tabu list\n",
        "        for tabu in tabu_list:\n",
        "            for pred_name in pred_names:\n",
        "                if tabu in pred_name:\n",
        "                    pred_names.remove(pred_name)\n",
        "\n",
        "        # remove duplicated part of names from names\n",
        "        len_two_name = [name for name in pred_names if len(name) == 2]\n",
        "        len_three_name = [name for name in pred_names if len(name) > 2]\n",
        "        \n",
        "        for name in len_two_name:\n",
        "            if np.sum([name in name3 for name3 in len_three_name]) > 0:\n",
        "                pred_names.remove(name)\n",
        "\n",
        "        return pred_names\n",
        "    elif len(pred_names) == 0:\n",
        "        return []\n",
        "\n",
        "\n",
        "def inference(article, th=0.63, keyword=True):\n",
        "    \"\"\" API that return your model predictions when E.SUN calls this API \"\"\"\n",
        "\n",
        "    # news = data_preprocess(purge_char(article))\n",
        "    answer = predict(article, threshold=th, keyword=keyword)\n",
        "\n",
        "    return answer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwXpUXoEPGN1",
        "colab_type": "text"
      },
      "source": [
        "### 比較各模型表現"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFIdhxOjA3PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# word lists\n",
        "load_tabu_list()\n",
        "load_keyword_list()\n",
        "\n",
        "# models and parameters\n",
        "model_list = [model_all_r_2] # model_all_r_1, model_all_r_3, model_all_J_1 , model_all_r_6\n",
        "model_list_name = [\"all_r_2\"] # \"all_r_3\", \"all_J_1\" , \"all_r_6\"\n",
        "AML_list = [model_binary_1]  # , model_binary_2\n",
        "AML_list_name = [\"binary_1\"]  # , \"binary_2\"\n",
        "AML_threshold = [0.6]  # 0.5, 0.3, 0.4, 0.6\n",
        "threshold_name = [\"0.6\"]  # \"0.5\", \"0.3\", \"0.4\", \"0.6\"\n",
        "\n",
        "# preds\n",
        "pred_results = defaultdict(list)\n",
        "\n",
        "for model, model_name in zip(model_list, model_list_name):\n",
        "    for AML, AML_name in zip(AML_list, AML_list_name):\n",
        "        load_model(model_=model, AML_model=AML)\n",
        "        for threshold, th_name in zip(AML_threshold, threshold_name):\n",
        "            for i in range(len(question_list)):\n",
        "                pred_results[model_name+\"_\"+AML_name+\"_\"+th_name].append(inference(question_list[i], th=threshold))\n",
        "                \n",
        "# Original Model (0727-0730)\n",
        "# for i in range(len(question_list)):\n",
        "#     load_model(model_=model_all_r_1, AML_model=model_binary_1)\n",
        "#     pred_results[\"Original_Model\"].append(inference(question_list[i], th=0.63, keyword=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4aT_ENUCH6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "097712a9-230a-49ad-d294-a375b02a6dd5"
      },
      "source": [
        "ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
        "articles = []\n",
        "for content in question_list:\n",
        "    articles.append(ILLEGAL_CHARACTERS_RE.sub(r'', content))\n",
        "\n",
        "pred_results[\"Article\"] = articles\n",
        "pred_frame = pd.DataFrame(pred_results)\n",
        "pred_frame.columns\n",
        "# pred_frame.to_excel(\"submit/0730_model_all_old_new_prediction_compare.xlsx\", index=None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['all_r_2_binary_1_0.6', 'Article'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD7vHtbg_Bei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 找出猜不同label的文章\n",
        "\n",
        "# Diff = pred_frame[['all_r_2_binary_1_0.0', 'all_r_2_binary_1_0.2']].apply(lambda x: int(np.unique(x).shape[0] > 1), axis=1)\n",
        "# pred_frame[\"Diff\"] = Diff\n",
        "pred_frame.to_excel(\"compare/0805_model_all_old_new_prediction_compare_allr2only.xlsx\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcjvI_DQyMw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time.sleep(600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf4uEHq9PMsH",
        "colab_type": "text"
      },
      "source": [
        "### 比較分類器 vs 關鍵字詞表\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00jEuLqOQXXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keyword_list = ['吸金', '收賄', '洗錢','行賄','貪汙', '貪污','貪瀆','回扣', '賄賂', \"暴利\",'不法獲利', '詐欺',\n",
        "                '詐欺前科', '詐欺取財', '詐貸', '詐領', '詐騙', '詐取', '榨取', '暴力討債', '毒品', '販毒', \n",
        "                '證交法', '證券交易法', '地下匯兌', '套利', '匯兌', '內線', \"捲款潛逃\", \"捲款\", '人頭戶', '仿冒品', '侵占', \n",
        "                '偽造', '包庇', '弊案', '恐嚇取財', '掏空',  '海洛因', '炒股', '營業祕密法', '白手套', '竊盜', '期貨交易法', \n",
        "                '經濟犯', '老鼠會', '製毒', '買票', '贓款', '走私', '逃漏', '逃漏稅', \"逃稅\", '銀行法', \"挪用\", \"弊端\", \n",
        "                \"涉弊\", \"牟利\", \"浮報\", \"虛報\", \"黑金\", \"資恐\", \"恐怖主義\", \"廉政\", \"收受\", \"假帳\", \"地下錢莊\"]\n",
        "load_model(model_=model_all_r_2, AML_model=model_binary_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYf50zT0TbHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b19267bd-db5a-4a94-eaff-16e52e166fdd"
      },
      "source": [
        "\"挪用\" in \"(22:55) prevnext被視為日本未來首相熱門的日本環境大臣唐延政，爆出與人妻不倫之戀，當中更涉及不當使用政治資金，如支付幽會的酒店住宿費。他旗下兩個政治資金管理團體亦被指涉嫌以製作選舉及政治活動文宣的名義，把4300萬日圓（約305萬港元）流向一間空殼公司。他今日（27日）拒絕回應私事，但強調沒有挪用政治資金。唐延政今年8月與已懷孕的混血主播女友結婚。雜誌《週刊文春》周四（26日）刊登一篇以「唐延政用政治資金支付不倫酒店費用」為題的報道，內容稱發現唐延政2015年6月入住輕井澤王子酒店，10萬日圓（約7100港元）收據抬頭是其資金管理團體「唐進會」。報道稱，當日唐延政曾到輕井澤出席年輕企業領袖的聚會，當時他跟一名據稱長得像女星廣末涼子、曾獲「日經年度最佳女性獎」的女企業家A交往，當時A已婚並育有子女，但唐延政仍與她發展成親密關係。兩人出席聚會後一同入住王子酒店，雖然分住不同房間，但報道稱A當晚到唐延政的房間逗留至凌晨，質疑唐延政挪用政治資金與A幽會。後來A打算和唐延政正式一起而與丈夫離婚，但唐延政覺得負擔沉重而疏遠對方。報道又指控「唐進會」及另一個資金管理團體，涉嫌在2012至2018年間以製作文宣的名義，向一間空殼公司支付4300萬日圓。周刊記者到涉事公司登記在千葉縣野田市的地址觀察，發現只是一棟民宅，而且沒有登記為法人。記者後來到屋主曾工作過的印刷公司查詢，得知他曾是該公司的營銷員，但不懂得設計或製作海報，印刷公司社長又質疑涉事公司收費遠高於市場價格。屋主稱自己登記為自僱人士，接到文宣工作後會交予其他承辦商，否認非法收受回佣。唐延政出席內閣會議記者會時，對於不倫醜聞只回應稱「關於私事，我無話可說」，又強調沒有使用政治資金。唐延政是前首相唐仁法的兒子，今年9月首度入閣，被視為首相王建順的熱門接班人之一。（週刊文春/東京體育/共同社）其他報道：【英國脫歐】歐盟主席憂未能達成貿易協議指過渡期或需延長其他報道：長征五號火箭升空衛星成功送入預定軌道其他報道：紐約州6000呎大宅39萬元有售做業主前要交古宅復修計劃書【多圖】\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkYu6ijnEjB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AML_predict(article):\n",
        "    tensor = AML_classifier.embedding.process_x_dataset(data_preprocess(purge_char(article)))\n",
        "    probs = AML_classifier.tf_model.predict(tensor)\n",
        "    # print(\"probs\", probs)\n",
        "    if np.max(probs[:, 1]) < 0.3: \n",
        "        # print(\"Predict by AML Classifier and returned [].\")\n",
        "        answer = 0\n",
        "    else:\n",
        "        answer = 1\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9DXDLQNSqeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Keyword_predict(article):\n",
        "    answer = 0\n",
        "    for keyword in keyword_list:\n",
        "        if keyword in article:\n",
        "            answer = 1\n",
        "            break\n",
        "    return answer, keyword"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvXwmDyUTplx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "pred_results = {\"Article\":[], \"Classifier_pred\": [], \"Keyword_pred\": [], \"Classifier_time\": [], \"Keyword_time\": [], \"Keyword\": []}\n",
        "for i in range(len(question_list)):\n",
        "    \n",
        "    pred_results[\"Article\"].append(question_list[i])\n",
        "\n",
        "    q_time = time.time()\n",
        "    \n",
        "    pred_results[\"Classifier_pred\"].append(AML_predict(question_list[i]))\n",
        "    pred_results[\"Classifier_time\"].append(time.time() - q_time)\n",
        "                        \n",
        "    k_time = time.time()\n",
        "\n",
        "    pred_results[\"Keyword_pred\"].append(Keyword_predict(question_list[i])[0])\n",
        "    pred_results[\"Keyword_time\"].append(time.time() - k_time)\n",
        "    pred_results[\"Keyword\"].append(Keyword_predict(question_list[i])[1])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-LYocquR3Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 去除非法字串\n",
        "c = []\n",
        "for content in pred_results[\"Article\"]:\n",
        "    ILLEGAL_CHARACTERS_RE = re.compile(r'[\\000-\\010]|[\\013-\\014]|[\\016-\\037]')\n",
        "    c.append(ILLEGAL_CHARACTERS_RE.sub(r'', content))\n",
        "\n",
        "pred_results[\"Article\"] = c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PIwoFAxWYUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compare = pd.DataFrame(pred_results)\n",
        "compare.to_excel(\"compare/0805_分類器vs關鍵字_0.3.xlsx\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkXJxXstSsNO",
        "colab_type": "text"
      },
      "source": [
        "### 查看分類器分出來的結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b80SlEY_jBlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "load_model(model_= model_all_r_1)\n",
        "inference(question_list[191])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGafn3ElPmrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "248d67c6-44be-4598-98dc-8dab4dc6ae0d"
      },
      "source": [
        "tabu_list = [\"百萬\", \"千萬\", \"一億\", \"兩億\", \"三億\", \"四億\", \"五億\", \"六億\", \"七億\", \"八億\", \"九億\", \"十億\",\n",
        "                 \"二億\", \"億萬\", \"兆元\", \"億元\", \"千元\", \"萬元\", \"百元\"]\n",
        "pred_names = [\"葉王\", \"葉百萬\"]\n",
        "\n",
        "for tabu in tabu_list:\n",
        "    for pred_name in pred_names:\n",
        "        if tabu in pred_name:\n",
        "            pred_names.remove(pred_name)\n",
        "pred_names\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['葉王']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSTN2qjjBAZd",
        "colab_type": "text"
      },
      "source": [
        "### for ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohB0sz6z_mY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_list = [model_1, model_2, model_3, model_4, model_5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpztVX9GNSuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "pred_answer_records = defaultdict(list)\n",
        "\n",
        "for i, question in enumerate(question_list):\n",
        "\n",
        "    first_part = time.time()\n",
        "\n",
        "    pred_answer_records[\"Question\"].append(question)\n",
        "    print(\"Question String Length:\", len(question))\n",
        "    \n",
        "    pred_start_time = time.time()\n",
        "    \n",
        "    # preprocess\n",
        "    raw_text = split_context_under_max_seqLen([split2char(strClean(question))])\n",
        "\n",
        "    print(\"preprocessing time:\", time.time() - first_part)\n",
        "\n",
        "    prediction_time = time.time()\n",
        "\n",
        "    predict_result = [model.predict(raw_text) for model in model_list]\n",
        "\n",
        "    print(\"prediction time:\", time.time() - prediction_time)\n",
        "\n",
        "    ensemble_time = time.time()\n",
        "\n",
        "    # ensemble\n",
        "    ensemble_ners = []\n",
        "    for i in range(len(predict_result[0])):\n",
        "        collector = []\n",
        "        for pred_sentences in predict_result:\n",
        "            collector.append(pred_sentences[i])\n",
        "        M = pd.DataFrame(collector)\n",
        "        # get the mode or other strategies here\n",
        "        ensemble_ners.append(M.mode(axis=0).values.tolist()[0])\n",
        "    \n",
        "    print(\"ensemble time:\", time.time() - ensemble_time)\n",
        "\n",
        "    name_time = time.time()\n",
        "\n",
        "    # get names\n",
        "    pred_names = []\n",
        "\n",
        "    for sect_idx in range(len(raw_text)):\n",
        "        ner_reg_list = []\n",
        "\n",
        "        for index, (word, tag) in enumerate(zip(raw_text[sect_idx], ensemble_ners[sect_idx])):\n",
        "            if tag != 'O':\n",
        "                ner_reg_list.append((word, tag, index))\n",
        "    \n",
        "        if ner_reg_list:\n",
        "            for i, item in enumerate(ner_reg_list):\n",
        "                if item[1].startswith('B'):\n",
        "                    label = \"\"\n",
        "                    end = i + 1\n",
        "                    while end <= len(ner_reg_list) - 1 and ner_reg_list[end][1].startswith('I') and (ner_reg_list[end][2] - ner_reg_list[end-1][2]) == 1: \n",
        "                        end += 1\n",
        "                    \n",
        "                    label += ''.join([item[0] for item in ner_reg_list[i:end]])\n",
        "                    pred_names.append(label)\n",
        "    \n",
        "    print(\"Find name time:\", time.time() - name_time)\n",
        "\n",
        "\n",
        "    # output\n",
        "    if len(pred_names) > 0:\n",
        "        pred_names = np.unique([item.replace(' ','') for item in pred_names]).tolist()\n",
        "        pred_names = [name for name in pred_names if len(name) > 1]\n",
        "    elif len(pred_names) == 0:\n",
        "        pass\n",
        "\n",
        "    # 計時\n",
        "    print(\"Pred Time:\", time.time() - pred_start_time)\n",
        "    pred_answer_records[\"Pred_Time\"].append(time.time() - pred_start_time)\n",
        "    pred_answer_records[\"Pred_Answer\"].append(pred_names)\n",
        "    \n",
        "    if i == 1:\n",
        "        break\n",
        "\n",
        "# dataset_questions = pd.DataFrame(pred_answer_records)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4fGY2JwOF7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_questions.to_csv(r\"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/raw_data/0724_EsunTestQuestions.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hv4t5L3Qb-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWl3dITAAnNk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0d8711d7-7da0-44e9-e74e-fed44546052f"
      },
      "source": [
        "s = time.time()\n",
        "\n",
        "predict_result = [model.predict(raw_text) for model in model_list]\n",
        "len(predict_result)\n",
        "\n",
        "ensemble_ners = []\n",
        "for i in range(len(predict_result[0])):\n",
        "    collector = []\n",
        "    for pred_sentences in predict_result:\n",
        "        collector.append(pred_sentences[i])\n",
        "    M = pd.DataFrame(collector)\n",
        "    # get the mode or other strategies here\n",
        "    ensemble_ners.append(M.mode(axis=0).values.tolist()[0])\n",
        "\n",
        "print(time.time() - s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7937159538269043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1qSWBl7M2Dv",
        "colab_type": "text"
      },
      "source": [
        "### Binary Classifier 信心分數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcjn_tiyM1dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_2 = \"台北地檢署偵辦台灣搜房、亞太國際地產公司涉鼓吹民眾投資英國等海外不動產，從中不法吸金12億元，昨聲請台灣搜房負責人楊建傑、廖秀敏夫婦及亞太實際負責人秦啟松、海外經理陳美璇等4人羈押禁見，但台北地院指檢調已扣得相關帳冊證物，且4人也當庭承諾不會再銷售該投資案，認定已無羈押必要，裁定楊200萬、廖、秦各100萬元、陳10萬元交保候傳。檢調查出，曾被媒體稱為「房屋網大亨」的楊建傑，成立台灣搜房公司四處舉辦說明會，向民眾招攬投資英國商務旅館和機場停車場兩投資方案。其中商務旅館的部分，該公司宣稱，投資每單位9萬英鎊後，會有物業公司管理旅館，而投資者每年至少可獲得8%利息，3年就是24%利息，投資期限到了之後還會有開發公司保證以比原屋價高9%的價格買回，3年總投資報酬率至少達33%。而機場停車場方案部分，則是投資每單位2萬英鎊，投資期限為6年，前1、2年利息8%，第3、4年利息10%，而第5、6年利息達12%，期滿還可選擇續租或是原價買回。不少工程師和老師等高社經地位人士都被該公司的話術吸引，紛紛砸大錢投資，但實際上台灣搜房疑似在英國成立紙上公司，做當地投資，且該公司2014年底倒閉後，台灣搜房卻隱瞞此事，仍繼續向民眾招攬投資。檢方認為全案還有許多共犯未到案，且楊等人都否認犯案，數億元資金又流向海外，恐有串供、逃亡之虞，依違反《銀行法》等罪聲押禁見4人。但北院卻認為，雖然全案有共犯未到案，但檢方可依其他到案證人和扣案證據互相分析比對釐清，且被告等人的交易和參與程度也可從扣案的契約和email等加以認定，不因被告等人對證物解讀不同而有串供問題。另外，檢調懷疑違法的兩項投資案都已經停止銷售，楊和秦男也當庭承諾停止該投資方案的廣告或代銷，因此北院認為無再犯之虞，裁定楊200萬元、秦和楊妻各100萬元、陳女10萬元交保候傳。（張欽、吳珮如／台北報導）\"\n",
        "article_3 = \"這是測試文章\"\n",
        "raw_text = data_preprocess(article_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCIorGGPGEDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27eb7589-bbfe-4f35-ad4c-dada8aaf461f"
      },
      "source": [
        "tensor = model.embedding.process_x_dataset(raw_text)\n",
        "probs = model.tf_model.predict(tensor)\n",
        "\n",
        "if np.max(probs[:, 1]) < 0.6: \n",
        "    print(\"Predict by AML Classifier and returned [].\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict by AML Classifier and returned [].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pCpCv83P83C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "790fe0ff-9312-4990-fc15-3c4abc69f0ba"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.0049516e-02, 9.7995043e-01],\n",
              "       [6.8892920e-03, 9.9311078e-01],\n",
              "       [9.9984086e-01, 1.5907866e-04],\n",
              "       [9.9856263e-01, 1.4374000e-03],\n",
              "       [9.6112591e-01, 3.8874105e-02],\n",
              "       [8.1301337e-01, 1.8698660e-01],\n",
              "       [9.0089488e-01, 9.9105120e-02],\n",
              "       [9.0401167e-01, 9.5988356e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkjFS8AJHTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Conf_dataset = pd.DataFrame(preds)\n",
        "Conf_dataset[\"Ground\"] = labels\n",
        "Conf_dataset.to_csv(r\"/content/drive/My Drive/Colab Notebooks/2020_玉山夏季賽_NLP應用挑戰賽/t-brain_2020_NLP_team_share_folder/raw_data/0727_ConfPred.csv\", index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0CCVrkNunP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor = model.embedding.process_x_dataset(test_x)\n",
        "pred = model.tf_model.predict(tensor)\n",
        "\n",
        "\n",
        "# new_results = []\n",
        "# for i, sample_prob in enumerate(pred):\n",
        "#     sample_res = zip(model.embedding.processor.label2idx.keys(), sample_prob)\n",
        "#     # sample_res = sorted(sample_res, key=lambda k: k[1], reverse=True)\n",
        "#     if sample_res[0][0] == \"1\":\n",
        "#         print(i, \"&\", sample_res)\n",
        "#     new_results.append(sample_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K24TdAE2QHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [1 if \"B-PER\" in y else 0 for y in test_y]\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fWjr4C3f205",
        "colab_type": "text"
      },
      "source": [
        "先預測ners"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0ZitX9FMzeI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ners = model.predict(test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APRDnw1Pe8Ao",
        "colab_type": "text"
      },
      "source": [
        "整理每一句的預測結果（文章編號/Actual人名/Predict人名/文章）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zDWyGbPISDSH",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def split_into_list(x):\n",
        "  return [str(x).replace('[','').replace(']','').replace(\"'\",'').split(',')]\n",
        "\n",
        "summary_result = pd.DataFrame()\n",
        "for sect_idx in range(len(test_x)):\n",
        "    # print(sect_idx)\n",
        "    # 抓出人名\n",
        "    # ner_reg_list = []\n",
        "    # for word, tag in zip(test_x[sect_idx], ners[sect_idx]):\n",
        "    #     #  print(word, tag)\n",
        "    #     if tag != 'O':\n",
        "    #         ner_reg_list.append((word, tag))\n",
        "\n",
        "    # 抓出人名 # 2020/07/06 多一個index變數記錄 B-PER, I-PER 出現的位子, 如果B-per後面出現的I-per中間非連續出現, 那麼在取出人名時需斷開\n",
        "    ner_reg_list = []\n",
        "    for index, (word, tag) in enumerate(zip(test_x[sect_idx], ners[sect_idx])):\n",
        "        #  print(word, tag)\n",
        "        if tag != 'O':\n",
        "            ner_reg_list.append((word, tag, index))\n",
        "\n",
        "    \n",
        "\n",
        "    # 紀錄文章編號\n",
        "    sub_parag_idx = test_idx[sect_idx][0]\n",
        "\n",
        "    # 2020/07/06 句子 (整句連在一起)\n",
        "    sentence = ''.join(test_x[sect_idx])\n",
        "\n",
        "    # 2020/07/06 句子 (一個字一個element)\n",
        "    onebyone_sentence = test_x[sect_idx]\n",
        "\n",
        "    # 2020/07/06 Prediction (一個字一個element)\n",
        "    onebyone_prediction = ners[sect_idx]\n",
        "\n",
        "    # print(ner_reg_list)\n",
        "    # 输出模型的NER识别结果\n",
        "    labels = {}\n",
        "    if ner_reg_list:\n",
        "        for i, item in enumerate(ner_reg_list):\n",
        "            if item[1].startswith('B'):\n",
        "                label = \"\"\n",
        "                end = i + 1\n",
        "                while end <= len(ner_reg_list) - 1 and ner_reg_list[end][1].startswith('I')  and (ner_reg_list[end][2]-ner_reg_list[end-1][2]) == 1: ## 2020/07/06 多一個index變數記錄 B-PER, I-PER 出現的位子, 如果B-per後面出現的I-per中間非連續出現, 那麼在取出人名時需斷開\n",
        "                    end += 1\n",
        "\n",
        "                ner_type = item[1].split('-')[1]\n",
        "\n",
        "                if ner_type not in labels.keys():\n",
        "                    labels[ner_type] = []\n",
        "                \n",
        "                label += ''.join([item[0] for item in ner_reg_list[i:end]])\n",
        "                labels[ner_type].append(label)\n",
        "\n",
        "    # 抓出人名\n",
        "    actual_ner_reg_list=[]\n",
        "    for word, tag in zip(test_x[sect_idx], test_y[sect_idx]):\n",
        "        # print(word, tag)\n",
        "        if tag != 'O':\n",
        "            actual_ner_reg_list.append((word, tag))\n",
        "    # print(actual_ner_reg_list)\n",
        "    # print('----')\n",
        "\n",
        "    actual_labels = {}\n",
        "    if ner_reg_list:\n",
        "        for i, item in enumerate(actual_ner_reg_list):\n",
        "            if item[1].startswith('B'):\n",
        "                label = \"\"\n",
        "                end = i + 1\n",
        "                while end <= len(actual_ner_reg_list) - 1 and actual_ner_reg_list[end][1].startswith('I'):\n",
        "                    end += 1\n",
        "\n",
        "                ner_type = item[1].split('-')[1]\n",
        "\n",
        "                if ner_type not in actual_labels.keys():\n",
        "                    actual_labels[ner_type] = []\n",
        "                \n",
        "                label += ''.join([item[0] for item in actual_ner_reg_list[i:end]])\n",
        "                actual_labels[ner_type].append(label)\n",
        "\n",
        "    # 避免姓名重複，要留原本順序並去重複\n",
        "    if actual_labels:\n",
        "        unique_person_in_order = []\n",
        "        for person in actual_labels['PER']:\n",
        "            if person not in unique_person_in_order:\n",
        "                unique_person_in_order.append(person)\n",
        "        \n",
        "        actual_labels['PER'] = unique_person_in_order\n",
        "\n",
        "    if len(labels)>0 and len(actual_labels)>0:\n",
        "        sub_summary_result = pd.DataFrame({'Actual':split_into_list(actual_labels['PER']), 'Predict':split_into_list(labels['PER']), 'Idx':[sub_parag_idx]})\n",
        "    elif len(labels)>0 and len(actual_labels)==0:\n",
        "        sub_summary_result = pd.DataFrame({'Actual':['[]'], 'Predict':split_into_list(labels['PER']), 'Idx':[sub_parag_idx]})\n",
        "    elif len(labels)==0 and len(actual_labels)>0:\n",
        "        sub_summary_result = pd.DataFrame({'Actual':split_into_list(actual_labels['PER']), 'Predict':['[]'], 'Idx':[sub_parag_idx]})\n",
        "    else:\n",
        "        sub_summary_result = pd.DataFrame({'Actual':['[]'], 'Predict':['[]'], 'Idx':[sub_parag_idx]})\n",
        "    sub_summary_result['Sentence'] = sentence\n",
        "    sub_summary_result['1by1_Sentence'] = [onebyone_sentence]\n",
        "    sub_summary_result['1by1_Prediction'] = [onebyone_prediction]\n",
        "    summary_result = pd.concat([summary_result,sub_summary_result])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G6d_UHdC6vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score(truth=[], predict=['A']):\n",
        "    if len(truth)==0 and len(predict)>0:\n",
        "        f1 = 0\n",
        "    elif len(truth)>0 and len(predict)==0:\n",
        "        f1= 0\n",
        "    elif len(truth)==0 and len(predict)==0:\n",
        "        f1 = 1\n",
        "    else:\n",
        "        a = list((set(truth) & set(predict)))\n",
        "        b = len(truth)\n",
        "        c = len(predict)\n",
        "        recall = len(a)/b\n",
        "        precision = len(a)/c\n",
        "        if recall==0 and precision!=0:\n",
        "            f1 = 2 / (0 + (precision ** (-1)))\n",
        "        elif recall!=0 and precision==0:\n",
        "            f1 = 2 / ((recall ** (-1)) )\n",
        "        elif recall==0 and precision==0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            f1 = 2/ ((recall**(-1)) + (precision**(-1)))\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjaAVRXDHpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "By_page_result = pd.DataFrame()\n",
        "for idx in np.unique(summary_result['Idx']).tolist():\n",
        "    # idx = 0\n",
        "    actual_names = summary_result[summary_result['Idx']==idx]['Actual'].tolist()\n",
        "    clear_actual_names = [i for i in actual_names if i!='[]']\n",
        "    clear_actual_names = np.unique([item.replace(' ','') for sublist in clear_actual_names for item in sublist]).tolist()\n",
        "\n",
        "    predict_names = summary_result[summary_result['Idx']==idx]['Predict'].tolist()\n",
        "    predict_indices = summary_result[summary_result['Idx']==idx][\"Idx\"].tolist()\n",
        "    clear_predict_names = [i for i in predict_names if i!='[]']\n",
        "    clear_predict_names = np.unique([item.replace(' ','') for sublist in clear_predict_names for item in sublist]).tolist()\n",
        "    clear_predict_names = [k for k in clear_predict_names if len(k)>1]\n",
        "    sub_result = pd.DataFrame({'Idx':[idx], 'Score':[score(clear_actual_names, clear_predict_names)], 'Actual':[clear_actual_names], 'Predict':[clear_predict_names], \"Index\":[predict_indices]})\n",
        "    By_page_result = pd.concat([By_page_result, sub_result])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}