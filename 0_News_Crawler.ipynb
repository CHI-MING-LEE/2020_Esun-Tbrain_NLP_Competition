{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0_News_Crawler.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"g2_iuxFG6nhm","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","\n","drive.mount('/content/drive')\n","os.chdir(r\"/content/drive/My Drive/Colab Notebooks/t-brain_2020_NLP_team_share_folder\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h9GXG63BmPnZ","colab_type":"text"},"source":["## News URLs"]},{"cell_type":"code","metadata":{"id":"7OPYyKHrmHnP","colab_type":"code","colab":{}},"source":["news_urls = pd.read_csv('raw_data/tbrain_train_final_0610.csv')\n","# news_urls = pd.read_csv('raw_data/train_data_custom_0626.csv')\n","news_urls.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1yEqReXuhLj","colab_type":"code","colab":{}},"source":["news_sources = np.unique(news_urls['hyperlink'].apply(lambda x:x.split('/')[2])).tolist()\n","print(\"共\", len(news_sources), \"個新聞網站來源\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSwTvMV-uhEW","colab_type":"text"},"source":["## Crawl Function"]},{"cell_type":"code","metadata":{"id":"zohSOH338B2U","colab_type":"code","colab":{}},"source":["# 認status code == 200\n","def get_url_status_code(url):\n","    return requests.get(url).status_code\n","\n","def crawl_news(url):\n","\n","    domain = url.split(\"/\")[2]\n","    special = False\n","\n","    if get_url_status_code(url) != 200:\n","        return [], []\n","\n","    # get the news\n","    res = requests.get(f'{url}')\n","\n","    if domain in [\"news.mingpao.com\", \"www.coolloud.org.tw\", \"hk.on.cc\"]:\n","        res.encoding = 'UTF-8'\n","\n","    if domain in [\"news.tvbs.com.tw\"]:\n","        soup = BeautifulSoup(res.text.replace(\"<br>\", \"<p>\"))\n","    else:\n","        soup = BeautifulSoup(res.text)\n","\n","    # get the title\n","    try:\n","        title = soup.find('h1').text\n","    except AttributeError as e:\n","        print(\"URL:\", url, \"title extraction failed.\")\n","        title = []\n","        \n","    try:\n","        # get the content\n","        if domain == 'mops.twse.com.tw':\n","            contents = soup.find_all('td', attrs={'style':\"text-align:left !important;\"})\n","            special = True\n","        elif domain == \"domestic.judicial.gov.tw\":\n","            contents = soup.find_all('pre')\n","        elif domain == \"udn.com\":\n","            contents = soup.find('section', attrs={'class':'article-content__editor'})\n","            clean_text = re.sub('[\\(\\)\\{\\}<>\\n\\t\\xa0\\r\\u3000]', '', contents.text)\n","            return title, clean_text\n","        elif domain == \"sina.com.hk\":\n","            contents = soup.find_all('p')\n","            title = contents[0].text\n","            special = True\n","        elif domain in [\"www.hk01.com\", \"m.ctee.com.tw\", \"house.ettoday.net\", \"ec.ltn.com.tw\", \"www.wealth.com.tw\"]:\n","            contents = soup.find_all('p')\n","            special = True\n","        elif domain == 'www.nownews.com':\n","            contents = soup.find_all('div','newsMsg')\n","            special = True\n","        elif domain in [\"www.bnext.com.tw\"]:\n","            try:\n","                contents = soup.find_all('div', attrs={\"data-url\": url})[0].find_all('p')\n","            except:\n","                contents = soup.find_all('p')\n","        elif domain in [\"hk.on.cc\"]:\n","            contents = soup.find_all('div', attrs={\"class\": \"paragraph\"})\n","            special = True\n","\n","        elif domain in [\"www.storm.mg\"]:\n","            contents = soup.find_all('div', attrs={\"id\": \"CMS_wrapper\", \"class\": \"article_content_inner\"})[0].find_all('p')\n","            special = True\n","        else:\n","            contents = soup.find_all('p')\n","    except:\n","        print(\"URL:\", url, \"contents extraction failed.\")\n","        return [], []\n","\n","    # join the content\n","    sentences = []\n","    for content in contents:\n","\n","        if domain in [\"m.ltn.com.tw\", \"ec.ltn.com.tw\", \"news.ltn.com.tw\"]:\n","            try:\n","                if content.attrs[\"class\"] == [\"appE1121\"]:\n","                    break\n","            except:\n","                pass\n","\n","        # if there is not a pure news content, skip it\n","        if len(content.attrs) == 0 or special:\n","\n","            if domain in [\"m.ctee.com.tw\", \"news.tvbs.com.tw\"]:\n","                try:\n","                    content.find('a').text\n","                    continue\n","                except:\n","                    pass\n","            \n","            if domain in [\"www.wealth.com.tw\"]:\n","                try:\n","                    attrs = content.find(\"a\").text\n","                    break\n","                except:\n","                    pass\n","\n","            if domain in [\"technews.tw\", \"money.udn.com\", \"estate.ltn.com.tw\", \"ccc.technews.tw\", \"finance.technews.tw\"]:\n","                if len(content.find_all('span')) > 0 or len(content.find_all('a')) > 0:\n","                    continue\n","\n","            # extract text and do basic clean\n","            clean_text = re.sub('[\\(\\)\\{\\}<>\\n\\t\\xa0\\r\\u3000]', '', content.text)\n","            sentences.append(clean_text)\n","            \n","            if domain in [\"udn.com\"]:\n","                try:\n","                    attrs = content.find_all(\"figure\")[0].attrs\n","                    if len(attrs) != 0:\n","                        break\n","                except:\n","                    pass\n","\n","        if domain in [\"www.hbrtaiwan.com\"]:\n","            if len(content.attrs) != 0:\n","                break\n","\n","    article = \"\".join(sentences).strip()\n","    \n","    return title, article\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfZKMWXJWw5M","colab_type":"code","colab":{}},"source":["for i in range(news_urls.shape[0]):\n","\n","    url = news_urls.loc[i, \"hyperlink\"]\n","\n","    # if url.split(\"/\")[2] in reload_domains:\n","\n","    title, content = crawl_news(url)\n","    # print(title, content)\n","    # titles.append(title)\n","    # contents.append(content)\n","\n","    print(f\"No. {i}th news with URL:{url} has been crawled.\")\n","    # time.sleep(1)\n","    if content is not None:\n","        news_urls.loc[i, \"title\"] = str(title)\n","        news_urls.loc[i, \"content\"] = str(content)\n","    else:\n","        print(\"content is None. Remain original content.\")"],"execution_count":null,"outputs":[]}]}